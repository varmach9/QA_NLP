{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_Q_A.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/varmach9/QA_NLP/blob/main/NLP_Q_A.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TlP2HPbXK3gj",
        "outputId": "65ca8ef7-0b48-47b3-aa6d-49450c8fdbed"
      },
      "source": [
        "!git clone https://github.com/susmitmishra125/QA_NLP"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'QA_NLP'...\n",
            "remote: Enumerating objects: 109, done.\u001b[K\n",
            "remote: Counting objects: 100% (109/109), done.\u001b[K\n",
            "remote: Compressing objects: 100% (84/84), done.\u001b[K\n",
            "remote: Total 109 (delta 55), reused 47 (delta 20), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (109/109), 270.17 KiB | 3.51 MiB/s, done.\n",
            "Resolving deltas: 100% (55/55), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2FVm2TnyVV0M",
        "outputId": "b2949ad1-9433-4f91-c435-05d085e14b97"
      },
      "source": [
        "!pip install contractions\n",
        "!pip install transformers"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting contractions\n",
            "  Downloading contractions-0.0.58-py2.py3-none-any.whl (8.0 kB)\n",
            "Collecting textsearch>=0.0.21\n",
            "  Downloading textsearch-0.0.21-py2.py3-none-any.whl (7.5 kB)\n",
            "Collecting pyahocorasick\n",
            "  Downloading pyahocorasick-1.4.2.tar.gz (321 kB)\n",
            "\u001b[K     |████████████████████████████████| 321 kB 5.3 MB/s \n",
            "\u001b[?25hCollecting anyascii\n",
            "  Downloading anyascii-0.3.0-py3-none-any.whl (284 kB)\n",
            "\u001b[K     |████████████████████████████████| 284 kB 33.3 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyahocorasick\n",
            "  Building wheel for pyahocorasick (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyahocorasick: filename=pyahocorasick-1.4.2-cp37-cp37m-linux_x86_64.whl size=85455 sha256=2c56e18fdf57826b79aa088211682cef45c0068adc2edc84963d21c5061e65a4\n",
            "  Stored in directory: /root/.cache/pip/wheels/25/19/a6/8f363d9939162782bb8439d886469756271abc01f76fbd790f\n",
            "Successfully built pyahocorasick\n",
            "Installing collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.0 contractions-0.0.58 pyahocorasick-1.4.2 textsearch-0.0.21\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.12.5-py3-none-any.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 36.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.1.2-py3-none-any.whl (59 kB)\n",
            "\u001b[K     |████████████████████████████████| 59 kB 6.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 42.9 MB/s \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 36.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.1.2 pyyaml-6.0 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.12.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-UdxUP1rTmP",
        "outputId": "60829ee1-4483-4d2c-fe4c-3b52ea0c6b69"
      },
      "source": [
        "%cd QA_NLP"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/QA_NLP\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25hyciYQrcU0",
        "outputId": "8e0e8fa4-3192-428a-977a-50a97be17266"
      },
      "source": [
        "# Importing modules\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from transformers import BertTokenizer\n",
        "from transformers import BertForQuestionAnswering\n",
        "import torch\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "import contractions\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "from collections import defaultdict\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "nltk.download('punkt')  # For tokenizers\n",
        "nltk.download('stopwords')\n",
        "\n",
        "\n",
        "# importing dataset\n",
        "data = open('passage.txt', 'r', encoding=\"utf-8\").read()\n",
        "questions = open('question_list.txt', 'r').readlines()\n",
        "\n",
        "# preprocess for modern method\n",
        "\n",
        "\n",
        "def preprocessing(rawReadCorpus, complete_preprocess=False):\n",
        "    pattern = \"^a-zA-Z0-9_\"\n",
        "    rawReadCorpus = contractions.fix(rawReadCorpus)\n",
        "    text_sent = sent_tokenize(rawReadCorpus)  # to split the sentences\n",
        "    text_sent = [sent.lower() for sent in text_sent]  # to convert to lowercase\n",
        "    text_sent = [\"\".join([char for char in text if char not in string.punctuation])\n",
        "                 for text in text_sent]  # removed punctuation\n",
        "    text_sent = [word_tokenize(sent) for sent in text_sent]\n",
        "    # text_sent = [\"\".join([char for char in text if char not in ]) for text in text_sent] # removed punctuation\n",
        "    for i in range(len(text_sent)):\n",
        "        sent = \" \".join(text_sent[i])\n",
        "        sent = re.sub(pattern, ' ', sent)\n",
        "        sent = sent.replace(\"“\", \" \")\n",
        "        sent = sent.replace(\"”\", \" \")\n",
        "        sent = sent.replace(\"—\", \" \")\n",
        "        sent = sent.replace(\"_\", \" \")\n",
        "        text_sent[i] = sent.split(' ')\n",
        "        text_sent[i] = [i for i in text_sent[i] if i != '']\n",
        "        # print(text_sent[i])\n",
        "    if complete_preprocess:\n",
        "        ps = nltk.porter.PorterStemmer()\n",
        "        for i in range(len(text_sent)):\n",
        "            text_sent[i] = [ps.stem(j) for j in text_sent[i]]\n",
        "    i = len(text_sent)-1\n",
        "    while(i >= 0):\n",
        "        if 'chapter' in text_sent[i]:\n",
        "            # print(text_sent[i])\n",
        "            del(text_sent[i])\n",
        "        i -= 1\n",
        "    return text_sent\n",
        "\n",
        "\n",
        "def answer_question(question, answer_text, model, tokenizer):\n",
        "    '''\n",
        "    Takes a question string and an answer_text string (which contains the\n",
        "    answer), and identifies the words within the answer_text that are the\n",
        "    answer. Prints them out.\n",
        "    '''\n",
        "    input_ids = tokenizer.encode(question, answer_text)\n",
        "    sep_index = input_ids.index(tokenizer.sep_token_id)\n",
        "    num_seg_a = sep_index + 1\n",
        "    num_seg_b = len(input_ids) - num_seg_a\n",
        "    segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
        "    assert len(segment_ids) == len(input_ids)\n",
        "    outputs = model(torch.tensor([input_ids]),  # The tokens representing our input text.\n",
        "                    # The segment IDs to differentiate question from answer_text\n",
        "                    token_type_ids=torch.tensor([segment_ids]),\n",
        "                    return_dict=True)\n",
        "    start_scores = outputs.start_logits\n",
        "    end_scores = outputs.end_logits\n",
        "    answer_start = torch.argmax(start_scores)\n",
        "    answer_end = torch.argmax(end_scores)\n",
        "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "    answer = tokens[answer_start]\n",
        "    for i in range(answer_start + 1, answer_end + 1):\n",
        "        if tokens[i][0:2] == '##':\n",
        "            answer += tokens[i][2:]\n",
        "        else:\n",
        "            answer += ' ' + tokens[i]\n",
        "    return (answer, start_scores[0][answer_start], end_scores[0][answer_end])\n",
        "\n",
        "\n",
        "def query(question, text_sent, model, tokenizer, n_sent=3):\n",
        "    outputs = []\n",
        "    sent_count = len(text_sent)\n",
        "    ans = 0\n",
        "    ans_text = 'NA'\n",
        "    for i in tqdm(range(sent_count-n_sent+1)):\n",
        "        segment = text_sent[i:i+n_sent]\n",
        "        passage = ''\n",
        "        for sent in segment:\n",
        "            passage += ' '.join(sent)+'. '\n",
        "        output = answer_question(question, passage, model, tokenizer)\n",
        "\n",
        "        outputs.append((output[0], 2*(output[1]*output[2])/(output[1]+output[2])))\n",
        "        if(len(outputs)>10):\n",
        "            outputs.sort(key=lambda x: x[1], reverse=True)\n",
        "            outputs=outputs[:10]\n",
        "\n",
        "    cnt = Counter()\n",
        "    for output in outputs:\n",
        "        cnt[output[0]] += 1\n",
        "    return cnt.most_common()[0][0]\n",
        "\n",
        "\n",
        "def modern():\n",
        "    text_sent = preprocessing(data)\n",
        "    model = BertForQuestionAnswering.from_pretrained(\n",
        "        'bert-large-uncased-whole-word-masking-finetuned-squad')\n",
        "    tokenizer = BertTokenizer.from_pretrained(\n",
        "        'bert-large-uncased-whole-word-masking-finetuned-squad')\n",
        "    for i in range(len(questions)):\n",
        "        # question in questions:\n",
        "        questions[i] = questions[i].replace('\\n', '')\n",
        "        print(questions[i])\n",
        "        ans = query(questions[i], text_sent=text_sent,model = model,tokenizer = tokenizer)\n",
        "        print(ans)\n",
        "\n",
        "from numpy import dot\n",
        "from numpy.linalg import norm\n",
        "\n",
        "# cos_sim = dot(a, b)/(norm(a)*norm(b))\n",
        "def traditional():\n",
        "    print(\"starting preprocessing\")\n",
        "    text_sent = preprocessing(data,complete_preprocess=False)\n",
        "    text_sent = [' '.join(sent) for sent in text_sent]\n",
        "    print(\"complete preprocessing\")\n",
        "    vectorizer = CountVectorizer(ngram_range=(1, 3))\n",
        "    \n",
        "    sentence_vectors = vectorizer.fit_transform(text_sent).toarray()\n",
        "\n",
        "    for question in questions:\n",
        "        ans = \"NA\"\n",
        "        max_cos = 0\n",
        "        q_vec = vectorizer.transform([question]).toarray()[0]\n",
        "        for i in range(len(text_sent)):\n",
        "            ans_vec = sentence_vectors[i]\n",
        "            cos_sim = dot(ans_vec,q_vec)/(norm(ans_vec)*norm(q_vec)+1e-9)\n",
        "            if cos_sim>max_cos:\n",
        "                # print(sum(q_vec),sum(ans_vec))\n",
        "                ans = text_sent[i]\n",
        "                max_cos = cos_sim\n",
        "        print(question)\n",
        "        print(ans,'\\n')\n",
        "    # print(sentence_vectors.toarr)\n",
        "# traditional()\n",
        "# modern()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vi8-JvFjzuP8"
      },
      "source": [
        "# This function runs the modern method on question present in passage.txt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YDYC-Yyyx7C"
      },
      "source": [
        "modern()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLp3KGYt0APR"
      },
      "source": [
        "# This function runs the traditional method on questions present in passage.txt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LIJf7r3y_Zt",
        "outputId": "dcce3d40-b956-4ed1-9258-4ae0d17c3c60",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "traditional()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "starting preprocessing\n",
            "complete preprocessing\n",
            "what is the colour of eyes of the white rabbit?\n",
            "\n",
            "down the rabbithole alice was beginning to get very tired of sitting by her sister on the bank and of having nothing to do once or twice she had peeped into the book her sister was reading but it had no pictures or conversations in it and what is the use of a book thought alice without pictures or conversations so she was considering in her own mind as well as she could for the hot day made her feel very sleepy and stupid whether the pleasure of making a daisychain would be worth the trouble of getting up and picking the daisies when suddenly a white rabbit with pink eyes ran close by her \n",
            "\n",
            "What is the label on the jar in the shelf?\n",
            "\n",
            "the duchess \n",
            "\n",
            "What was on the solid glass three legged table?\n",
            "\n",
            "the little door was shut again and the little golden key was lying on the glass table as before and things are worse than ever thought the poor child for i never was so small as this before never \n",
            "\n",
            "Who was splendidly dressed and carrying a pair of white kid gloves in one hand and a large fan in the other?\n",
            "\n",
            "it was the white rabbit returning splendidly dressed with a pair of white kid gloves in one hand and a large fan in the other he came trotting along in a great hurry muttering to himself as he came oh \n",
            "\n",
            "Who was afraid of being drowned in own tears?\n",
            "\n",
            "i shall be punished for it now i suppose by being drowned in my own tears \n",
            "\n",
            "What did Alice pull out of her pocket as prizes?\n",
            "\n",
            "prizes alice had no idea what to do and in despair she put her hand in her pocket and pulled out a box of comfits luckily the salt water had not got into it and handed them round as prizes \n",
            "\n",
            "What was written on the door of bright brass plate?\n",
            "\n",
            "but i would better take him his fan and gloves that is if i can find them as she said this she came upon a neat little house on the door of which was a bright brass plate with the name w rabbit engraved upon it \n",
            "\n",
            "Whom did Alice gave one sharp kick?\n",
            "\n",
            "poor alice \n",
            "\n",
            "What happened when Alice swallowed one of the cakes?\n",
            "\n",
            "if i eat one of these cakes she thought it is sure to make some change in my size and as it can not possibly make me larger it must make me smaller i suppose so she swallowed one of the cakes and was delighted to find that she began shrinking directly \n",
            "\n",
            "What was the large blue caterpillar doing?\n",
            "\n",
            "she stretched herself up on tiptoe and peeped over the edge of the mushroom and her eyes immediately met those of a large blue caterpillar that was sitting on the top with its arms folded quietly smoking a long hookah and taking not the smallest notice of her or of anything else \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yf0W7RG40M3a"
      },
      "source": [
        "# For Testing new questions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWVeh7PbrcSy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7aa4f9e1-f6cf-4f69-b3cd-05f39b8c7844"
      },
      "source": [
        "def answerquery():\n",
        "    text_sent = preprocessing(data)\n",
        "    model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
        "    while(True):\n",
        "        question = input(\"enter the question\\n\")\n",
        "        if question==\"end\":\n",
        "          break\n",
        "        ans = query(question, text_sent=text_sent,model = model,tokenizer = tokenizer)\n",
        "        print(\"Modern: \"+ans)\n",
        "        vectorizer = CountVectorizer(ngram_range=(1, 3))\n",
        "        text_sent = [' '.join(sent) for sent in text_sent]\n",
        "        sentence_vectors = vectorizer.fit_transform(text_sent).toarray()\n",
        "        ans = \"NA\"\n",
        "        max_cos = 0\n",
        "        q_vec = vectorizer.transform([question]).toarray()[0]\n",
        "        for i in range(len(text_sent)):\n",
        "            ans_vec = sentence_vectors[i]\n",
        "            cos_sim = dot(ans_vec,q_vec)/(norm(ans_vec)*norm(q_vec)+1e-9)\n",
        "            if cos_sim>max_cos:\n",
        "                # print(sum(q_vec),sum(ans_vec))\n",
        "                ans = text_sent[i]\n",
        "                max_cos = cos_sim\n",
        "        # print(question)\n",
        "        print(\"traditional\")\n",
        "        print(ans,'\\n')\n",
        "answerquery()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "enter the question\n",
            "What did Alice pull out of her pocket as prizes?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  0%|          | 0/289 [00:00<?, ?it/s]\u001b[A\n",
            "  0%|          | 1/289 [00:02<11:13,  2.34s/it]\u001b[A\n",
            "  1%|          | 2/289 [00:04<10:58,  2.30s/it]\u001b[A\n",
            "  1%|          | 3/289 [00:06<10:39,  2.23s/it]\u001b[A\n",
            "  1%|▏         | 4/289 [00:09<11:28,  2.42s/it]\u001b[A\n",
            "  2%|▏         | 5/289 [00:11<10:10,  2.15s/it]\u001b[A\n",
            "  2%|▏         | 6/289 [00:13<10:10,  2.16s/it]\u001b[A\n",
            "  2%|▏         | 7/289 [00:15<10:31,  2.24s/it]\u001b[A\n",
            "  3%|▎         | 8/289 [00:17<10:17,  2.20s/it]\u001b[A\n",
            "  3%|▎         | 9/289 [00:19<09:42,  2.08s/it]\u001b[A\n",
            "  3%|▎         | 10/289 [00:20<08:13,  1.77s/it]\u001b[A\n",
            "  4%|▍         | 11/289 [00:21<06:45,  1.46s/it]\u001b[A\n",
            "  4%|▍         | 12/289 [00:22<05:39,  1.22s/it]\u001b[A\n",
            "  4%|▍         | 13/289 [00:22<04:47,  1.04s/it]\u001b[A\n",
            "  5%|▍         | 14/289 [00:23<04:20,  1.06it/s]\u001b[A\n",
            "  5%|▌         | 15/289 [00:25<05:34,  1.22s/it]\u001b[A\n",
            "  6%|▌         | 16/289 [00:27<06:16,  1.38s/it]\u001b[A\n",
            "  6%|▌         | 17/289 [00:28<06:38,  1.46s/it]\u001b[A\n",
            "  6%|▌         | 18/289 [00:29<05:34,  1.23s/it]\u001b[A\n",
            "  7%|▋         | 19/289 [00:30<05:28,  1.22s/it]\u001b[A\n",
            "  7%|▋         | 20/289 [00:32<05:46,  1.29s/it]\u001b[A\n",
            "  7%|▋         | 21/289 [00:33<05:51,  1.31s/it]\u001b[A\n",
            "  8%|▊         | 22/289 [00:34<05:22,  1.21s/it]\u001b[A\n",
            "  8%|▊         | 23/289 [00:35<04:42,  1.06s/it]\u001b[A\n",
            "  8%|▊         | 24/289 [00:36<04:29,  1.02s/it]\u001b[A\n",
            "  9%|▊         | 25/289 [00:37<04:21,  1.01it/s]\u001b[A\n",
            "  9%|▉         | 26/289 [00:37<04:11,  1.04it/s]\u001b[A\n",
            "  9%|▉         | 27/289 [00:38<03:47,  1.15it/s]\u001b[A\n",
            " 10%|▉         | 28/289 [00:39<03:27,  1.26it/s]\u001b[A\n",
            " 10%|█         | 29/289 [00:39<03:22,  1.28it/s]\u001b[A\n",
            " 10%|█         | 30/289 [00:41<03:49,  1.13it/s]\u001b[A\n",
            " 11%|█         | 31/289 [00:42<04:27,  1.04s/it]\u001b[A\n",
            " 11%|█         | 32/289 [00:44<05:12,  1.22s/it]\u001b[A\n",
            " 11%|█▏        | 33/289 [00:45<05:15,  1.23s/it]\u001b[A\n",
            " 12%|█▏        | 34/289 [00:46<05:05,  1.20s/it]\u001b[A\n",
            " 12%|█▏        | 35/289 [00:47<04:57,  1.17s/it]\u001b[A\n",
            " 12%|█▏        | 36/289 [00:49<05:55,  1.40s/it]\u001b[A\n",
            " 13%|█▎        | 37/289 [00:51<07:02,  1.68s/it]\u001b[A\n",
            " 13%|█▎        | 38/289 [00:54<07:58,  1.91s/it]\u001b[A\n",
            " 13%|█▎        | 39/289 [00:55<07:41,  1.85s/it]\u001b[A\n",
            " 14%|█▍        | 40/289 [00:57<07:27,  1.80s/it]\u001b[A\n",
            " 14%|█▍        | 41/289 [00:59<07:08,  1.73s/it]\u001b[A\n",
            " 15%|█▍        | 42/289 [01:01<07:21,  1.79s/it]\u001b[A\n",
            " 15%|█▍        | 43/289 [01:02<07:05,  1.73s/it]\u001b[A\n",
            " 15%|█▌        | 44/289 [01:04<06:55,  1.70s/it]\u001b[A\n",
            " 16%|█▌        | 45/289 [01:06<07:08,  1.75s/it]\u001b[A\n",
            " 16%|█▌        | 46/289 [01:08<07:24,  1.83s/it]\u001b[A\n",
            " 16%|█▋        | 47/289 [01:11<08:47,  2.18s/it]\u001b[A\n",
            " 17%|█▋        | 48/289 [01:13<09:20,  2.32s/it]\u001b[A\n",
            " 17%|█▋        | 49/289 [01:16<10:04,  2.52s/it]\u001b[A\n",
            " 17%|█▋        | 50/289 [01:19<09:33,  2.40s/it]\u001b[A\n",
            " 18%|█▊        | 51/289 [01:20<08:51,  2.24s/it]\u001b[A\n",
            " 18%|█▊        | 52/289 [01:22<08:03,  2.04s/it]\u001b[A\n",
            " 18%|█▊        | 53/289 [01:24<08:00,  2.04s/it]\u001b[A\n",
            " 19%|█▊        | 54/289 [01:27<08:40,  2.21s/it]\u001b[A\n",
            " 19%|█▉        | 55/289 [01:29<08:55,  2.29s/it]\u001b[A\n",
            " 19%|█▉        | 56/289 [01:31<08:44,  2.25s/it]\u001b[A\n",
            " 20%|█▉        | 57/289 [01:33<08:10,  2.11s/it]\u001b[A\n",
            " 20%|██        | 58/289 [01:36<08:37,  2.24s/it]\u001b[A\n",
            " 20%|██        | 59/289 [01:38<08:23,  2.19s/it]\u001b[A\n",
            " 21%|██        | 60/289 [01:39<07:56,  2.08s/it]\u001b[A\n",
            " 21%|██        | 61/289 [01:41<07:01,  1.85s/it]\u001b[A\n",
            " 21%|██▏       | 62/289 [01:42<06:26,  1.70s/it]\u001b[A\n",
            " 22%|██▏       | 63/289 [01:43<05:37,  1.49s/it]\u001b[A\n",
            " 22%|██▏       | 64/289 [01:44<05:13,  1.39s/it]\u001b[A\n",
            " 22%|██▏       | 65/289 [01:46<05:06,  1.37s/it]\u001b[A\n",
            " 23%|██▎       | 66/289 [01:47<05:07,  1.38s/it]\u001b[A\n",
            " 23%|██▎       | 67/289 [01:48<04:41,  1.27s/it]\u001b[A\n",
            " 24%|██▎       | 68/289 [01:49<04:16,  1.16s/it]\u001b[A\n",
            " 24%|██▍       | 69/289 [01:50<04:16,  1.17s/it]\u001b[A\n",
            " 24%|██▍       | 70/289 [01:51<04:10,  1.15s/it]\u001b[A\n",
            " 25%|██▍       | 71/289 [01:53<04:27,  1.23s/it]\u001b[A\n",
            " 25%|██▍       | 72/289 [01:54<04:22,  1.21s/it]\u001b[A\n",
            " 25%|██▌       | 73/289 [01:55<04:45,  1.32s/it]\u001b[A\n",
            " 26%|██▌       | 74/289 [01:57<04:51,  1.35s/it]\u001b[A\n",
            " 26%|██▌       | 75/289 [01:58<05:05,  1.43s/it]\u001b[A\n",
            " 26%|██▋       | 76/289 [02:00<04:52,  1.37s/it]\u001b[A\n",
            " 27%|██▋       | 77/289 [02:01<04:24,  1.25s/it]\u001b[A\n",
            " 27%|██▋       | 78/289 [02:02<04:27,  1.27s/it]\u001b[A\n",
            " 27%|██▋       | 79/289 [02:03<04:41,  1.34s/it]\u001b[A\n",
            " 28%|██▊       | 80/289 [02:05<04:53,  1.40s/it]\u001b[A\n",
            " 28%|██▊       | 81/289 [02:06<04:21,  1.26s/it]\u001b[A\n",
            " 28%|██▊       | 82/289 [02:06<03:38,  1.06s/it]\u001b[A\n",
            " 29%|██▊       | 83/289 [02:07<03:13,  1.07it/s]\u001b[A\n",
            " 29%|██▉       | 84/289 [02:08<02:58,  1.15it/s]\u001b[A\n",
            " 29%|██▉       | 85/289 [02:09<02:58,  1.14it/s]\u001b[A\n",
            " 30%|██▉       | 86/289 [02:10<03:12,  1.05it/s]\u001b[A\n",
            " 30%|███       | 87/289 [02:11<03:49,  1.13s/it]\u001b[A\n",
            " 30%|███       | 88/289 [02:13<04:07,  1.23s/it]\u001b[A\n",
            " 31%|███       | 89/289 [02:14<04:07,  1.24s/it]\u001b[A\n",
            " 31%|███       | 90/289 [02:15<03:36,  1.09s/it]\u001b[A\n",
            " 31%|███▏      | 91/289 [02:16<03:31,  1.07s/it]\u001b[A\n",
            " 32%|███▏      | 92/289 [02:17<03:20,  1.02s/it]\u001b[A\n",
            " 32%|███▏      | 93/289 [02:18<03:11,  1.02it/s]\u001b[A\n",
            " 33%|███▎      | 94/289 [02:19<03:07,  1.04it/s]\u001b[A\n",
            " 33%|███▎      | 95/289 [02:19<03:02,  1.06it/s]\u001b[A\n",
            " 33%|███▎      | 96/289 [02:21<03:36,  1.12s/it]\u001b[A\n",
            " 34%|███▎      | 97/289 [02:23<04:31,  1.42s/it]\u001b[A\n",
            " 34%|███▍      | 98/289 [02:25<05:10,  1.62s/it]\u001b[A\n",
            " 34%|███▍      | 99/289 [02:27<05:05,  1.61s/it]\u001b[A\n",
            " 35%|███▍      | 100/289 [02:28<04:31,  1.44s/it]\u001b[A\n",
            " 35%|███▍      | 101/289 [02:29<04:35,  1.47s/it]\u001b[A\n",
            " 35%|███▌      | 102/289 [02:31<04:51,  1.56s/it]\u001b[A\n",
            " 36%|███▌      | 103/289 [02:33<04:50,  1.56s/it]\u001b[A\n",
            " 36%|███▌      | 104/289 [02:34<05:00,  1.62s/it]\u001b[A\n",
            " 36%|███▋      | 105/289 [02:36<05:05,  1.66s/it]\u001b[A\n",
            " 37%|███▋      | 106/289 [02:38<05:26,  1.79s/it]\u001b[A\n",
            " 37%|███▋      | 107/289 [02:40<05:15,  1.73s/it]\u001b[A\n",
            " 37%|███▋      | 108/289 [02:41<04:43,  1.57s/it]\u001b[A\n",
            " 38%|███▊      | 109/289 [02:42<04:15,  1.42s/it]\u001b[A\n",
            " 38%|███▊      | 110/289 [02:44<04:16,  1.43s/it]\u001b[A\n",
            " 38%|███▊      | 111/289 [02:45<04:23,  1.48s/it]\u001b[A\n",
            " 39%|███▉      | 112/289 [02:47<04:26,  1.50s/it]\u001b[A\n",
            " 39%|███▉      | 113/289 [02:48<04:02,  1.38s/it]\u001b[A\n",
            " 39%|███▉      | 114/289 [02:49<03:37,  1.24s/it]\u001b[A\n",
            " 40%|███▉      | 115/289 [02:50<03:50,  1.33s/it]\u001b[A\n",
            " 40%|████      | 116/289 [02:52<03:57,  1.38s/it]\u001b[A\n",
            " 40%|████      | 117/289 [02:54<04:22,  1.52s/it]\u001b[A\n",
            " 41%|████      | 118/289 [02:56<04:43,  1.66s/it]\u001b[A\n",
            " 41%|████      | 119/289 [02:58<05:09,  1.82s/it]\u001b[A\n",
            " 42%|████▏     | 120/289 [03:00<05:16,  1.87s/it]\u001b[A\n",
            " 42%|████▏     | 121/289 [03:01<04:47,  1.71s/it]\u001b[A\n",
            " 42%|████▏     | 122/289 [03:02<04:12,  1.51s/it]\u001b[A\n",
            " 43%|████▎     | 123/289 [03:03<03:47,  1.37s/it]\u001b[A\n",
            " 43%|████▎     | 124/289 [03:04<03:33,  1.29s/it]\u001b[A\n",
            " 43%|████▎     | 125/289 [03:06<03:26,  1.26s/it]\u001b[A\n",
            " 44%|████▎     | 126/289 [03:07<03:54,  1.44s/it]\u001b[A\n",
            " 44%|████▍     | 127/289 [03:09<04:18,  1.59s/it]\u001b[A\n",
            " 44%|████▍     | 128/289 [03:11<04:25,  1.65s/it]\u001b[A\n",
            " 45%|████▍     | 129/289 [03:12<03:51,  1.44s/it]\u001b[A\n",
            " 45%|████▍     | 130/289 [03:13<03:24,  1.28s/it]\u001b[A\n",
            " 45%|████▌     | 131/289 [03:14<03:15,  1.24s/it]\u001b[A\n",
            " 46%|████▌     | 132/289 [03:15<03:16,  1.25s/it]\u001b[A\n",
            " 46%|████▌     | 133/289 [03:17<03:29,  1.34s/it]\u001b[A\n",
            " 46%|████▋     | 134/289 [03:19<03:47,  1.47s/it]\u001b[A\n",
            " 47%|████▋     | 135/289 [03:20<03:53,  1.52s/it]\u001b[A\n",
            " 47%|████▋     | 136/289 [03:23<04:32,  1.78s/it]\u001b[A\n",
            " 47%|████▋     | 137/289 [03:25<04:39,  1.84s/it]\u001b[A\n",
            " 48%|████▊     | 138/289 [03:27<05:00,  1.99s/it]\u001b[A\n",
            " 48%|████▊     | 139/289 [03:29<04:38,  1.86s/it]\u001b[A\n",
            " 48%|████▊     | 140/289 [03:31<04:45,  1.92s/it]\u001b[A\n",
            " 49%|████▉     | 141/289 [03:33<04:41,  1.90s/it]\u001b[A\n",
            " 49%|████▉     | 142/289 [03:34<04:26,  1.82s/it]\u001b[A\n",
            " 49%|████▉     | 143/289 [03:35<04:00,  1.65s/it]\u001b[A\n",
            " 50%|████▉     | 144/289 [03:37<03:34,  1.48s/it]\u001b[A\n",
            " 50%|█████     | 145/289 [03:37<03:09,  1.31s/it]\u001b[A\n",
            " 51%|█████     | 146/289 [03:38<02:38,  1.11s/it]\u001b[A\n",
            " 51%|█████     | 147/289 [03:39<02:31,  1.07s/it]\u001b[A\n",
            " 51%|█████     | 148/289 [03:40<02:32,  1.08s/it]\u001b[A\n",
            " 52%|█████▏    | 149/289 [03:41<02:39,  1.14s/it]\u001b[A\n",
            " 52%|█████▏    | 150/289 [03:42<02:31,  1.09s/it]\u001b[A\n",
            " 52%|█████▏    | 151/289 [03:43<02:12,  1.04it/s]\u001b[A\n",
            " 53%|█████▎    | 152/289 [03:44<02:09,  1.06it/s]\u001b[A\n",
            " 53%|█████▎    | 153/289 [03:45<02:25,  1.07s/it]\u001b[A\n",
            " 53%|█████▎    | 154/289 [03:47<02:51,  1.27s/it]\u001b[A\n",
            " 54%|█████▎    | 155/289 [03:49<02:57,  1.32s/it]\u001b[A\n",
            " 54%|█████▍    | 156/289 [03:50<02:54,  1.31s/it]\u001b[A\n",
            " 54%|█████▍    | 157/289 [03:51<02:56,  1.33s/it]\u001b[A\n",
            " 55%|█████▍    | 158/289 [03:53<03:12,  1.47s/it]\u001b[A\n",
            " 55%|█████▌    | 159/289 [03:55<03:40,  1.70s/it]\u001b[A\n",
            " 55%|█████▌    | 160/289 [03:57<03:51,  1.79s/it]\u001b[A\n",
            " 56%|█████▌    | 161/289 [03:59<03:54,  1.83s/it]\u001b[A\n",
            " 56%|█████▌    | 162/289 [04:01<03:42,  1.75s/it]\u001b[A\n",
            " 56%|█████▋    | 163/289 [04:03<03:53,  1.86s/it]\u001b[A\n",
            " 57%|█████▋    | 164/289 [04:05<03:59,  1.92s/it]\u001b[A\n",
            " 57%|█████▋    | 165/289 [04:07<03:59,  1.93s/it]\u001b[A\n",
            " 57%|█████▋    | 166/289 [04:08<03:41,  1.80s/it]\u001b[A\n",
            " 58%|█████▊    | 167/289 [04:10<03:21,  1.65s/it]\u001b[A\n",
            " 58%|█████▊    | 168/289 [04:11<03:00,  1.49s/it]\u001b[A\n",
            " 58%|█████▊    | 169/289 [04:11<02:28,  1.23s/it]\u001b[A\n",
            " 59%|█████▉    | 170/289 [04:12<02:07,  1.07s/it]\u001b[A\n",
            " 59%|█████▉    | 171/289 [04:13<01:52,  1.05it/s]\u001b[A\n",
            " 60%|█████▉    | 172/289 [04:13<01:41,  1.16it/s]\u001b[A\n",
            " 60%|█████▉    | 173/289 [04:14<01:46,  1.09it/s]\u001b[A\n",
            " 60%|██████    | 174/289 [04:16<02:03,  1.07s/it]\u001b[A\n",
            " 61%|██████    | 175/289 [04:18<02:27,  1.29s/it]\u001b[A\n",
            " 61%|██████    | 176/289 [04:19<02:36,  1.39s/it]\u001b[A\n",
            " 61%|██████    | 177/289 [04:21<02:38,  1.41s/it]\u001b[A\n",
            " 62%|██████▏   | 178/289 [04:22<02:29,  1.35s/it]\u001b[A\n",
            " 62%|██████▏   | 179/289 [04:25<03:13,  1.76s/it]\u001b[A\n",
            " 62%|██████▏   | 180/289 [04:27<03:42,  2.04s/it]\u001b[A\n",
            " 63%|██████▎   | 181/289 [04:30<04:03,  2.25s/it]\u001b[A\n",
            " 63%|██████▎   | 182/289 [04:31<03:26,  1.93s/it]\u001b[A\n",
            " 63%|██████▎   | 183/289 [04:32<02:55,  1.65s/it]\u001b[A\n",
            " 64%|██████▎   | 184/289 [04:33<02:33,  1.47s/it]\u001b[A\n",
            " 64%|██████▍   | 185/289 [04:34<02:20,  1.35s/it]\u001b[A\n",
            " 64%|██████▍   | 186/289 [04:36<02:18,  1.35s/it]\u001b[A\n",
            " 65%|██████▍   | 187/289 [04:37<02:21,  1.38s/it]\u001b[A\n",
            " 65%|██████▌   | 188/289 [04:39<02:20,  1.39s/it]\u001b[A\n",
            " 65%|██████▌   | 189/289 [04:40<02:13,  1.34s/it]\u001b[A\n",
            " 66%|██████▌   | 190/289 [04:41<02:06,  1.28s/it]\u001b[A\n",
            " 66%|██████▌   | 191/289 [04:42<01:58,  1.20s/it]\u001b[A\n",
            " 66%|██████▋   | 192/289 [04:43<01:47,  1.11s/it]\u001b[A\n",
            " 67%|██████▋   | 193/289 [04:44<01:41,  1.06s/it]\u001b[A\n",
            " 67%|██████▋   | 194/289 [04:45<01:48,  1.14s/it]\u001b[A\n",
            " 67%|██████▋   | 195/289 [04:47<01:57,  1.25s/it]\u001b[A\n",
            " 68%|██████▊   | 196/289 [04:48<02:01,  1.31s/it]\u001b[A\n",
            " 68%|██████▊   | 197/289 [04:49<01:53,  1.24s/it]\u001b[A\n",
            " 69%|██████▊   | 198/289 [04:50<01:39,  1.09s/it]\u001b[A\n",
            " 69%|██████▉   | 199/289 [04:51<01:35,  1.06s/it]\u001b[A\n",
            " 69%|██████▉   | 200/289 [04:52<01:37,  1.09s/it]\u001b[A\n",
            " 70%|██████▉   | 201/289 [04:54<01:49,  1.24s/it]\u001b[A\n",
            " 70%|██████▉   | 202/289 [04:55<01:50,  1.27s/it]\u001b[A\n",
            " 70%|███████   | 203/289 [04:56<01:41,  1.18s/it]\u001b[A\n",
            " 71%|███████   | 204/289 [04:57<01:22,  1.03it/s]\u001b[A\n",
            " 71%|███████   | 205/289 [04:57<01:13,  1.15it/s]\u001b[A\n",
            " 71%|███████▏  | 206/289 [04:59<01:27,  1.05s/it]\u001b[A\n",
            " 72%|███████▏  | 207/289 [05:00<01:42,  1.25s/it]\u001b[A\n",
            " 72%|███████▏  | 208/289 [05:02<01:51,  1.37s/it]\u001b[A\n",
            " 72%|███████▏  | 209/289 [05:03<01:46,  1.33s/it]\u001b[A\n",
            " 73%|███████▎  | 210/289 [05:04<01:38,  1.25s/it]\u001b[A\n",
            " 73%|███████▎  | 211/289 [05:05<01:31,  1.18s/it]\u001b[A\n",
            " 73%|███████▎  | 212/289 [05:07<01:32,  1.20s/it]\u001b[A\n",
            " 74%|███████▎  | 213/289 [05:08<01:36,  1.27s/it]\u001b[A\n",
            " 74%|███████▍  | 214/289 [05:09<01:39,  1.32s/it]\u001b[A\n",
            " 74%|███████▍  | 215/289 [05:11<01:36,  1.30s/it]\u001b[A\n",
            " 75%|███████▍  | 216/289 [05:12<01:29,  1.22s/it]\u001b[A\n",
            " 75%|███████▌  | 217/289 [05:14<01:46,  1.48s/it]\u001b[A\n",
            " 75%|███████▌  | 218/289 [05:16<01:58,  1.67s/it]\u001b[A\n",
            " 76%|███████▌  | 219/289 [05:18<02:07,  1.83s/it]\u001b[A\n",
            " 76%|███████▌  | 220/289 [05:20<02:02,  1.77s/it]\u001b[A\n",
            " 76%|███████▋  | 221/289 [05:22<02:02,  1.80s/it]\u001b[A\n",
            " 77%|███████▋  | 222/289 [05:23<01:57,  1.75s/it]\u001b[A\n",
            " 77%|███████▋  | 223/289 [05:25<01:50,  1.68s/it]\u001b[A\n",
            " 78%|███████▊  | 224/289 [05:26<01:43,  1.60s/it]\u001b[A\n",
            " 78%|███████▊  | 225/289 [05:28<01:48,  1.70s/it]\u001b[A\n",
            " 78%|███████▊  | 226/289 [05:30<01:46,  1.69s/it]\u001b[A\n",
            " 79%|███████▊  | 227/289 [05:31<01:41,  1.65s/it]\u001b[A\n",
            " 79%|███████▉  | 228/289 [05:32<01:30,  1.48s/it]\u001b[A\n",
            " 79%|███████▉  | 229/289 [05:34<01:22,  1.37s/it]\u001b[A\n",
            " 80%|███████▉  | 230/289 [05:35<01:13,  1.25s/it]\u001b[A\n",
            " 80%|███████▉  | 231/289 [05:36<01:14,  1.29s/it]\u001b[A\n",
            " 80%|████████  | 232/289 [05:37<01:14,  1.30s/it]\u001b[A\n",
            " 81%|████████  | 233/289 [05:39<01:13,  1.32s/it]\u001b[A\n",
            " 81%|████████  | 234/289 [05:39<01:05,  1.19s/it]\u001b[A\n",
            " 81%|████████▏ | 235/289 [05:41<01:03,  1.18s/it]\u001b[A\n",
            " 82%|████████▏ | 236/289 [05:42<01:00,  1.15s/it]\u001b[A\n",
            " 82%|████████▏ | 237/289 [05:43<00:58,  1.13s/it]\u001b[A\n",
            " 82%|████████▏ | 238/289 [05:43<00:49,  1.03it/s]\u001b[A\n",
            " 83%|████████▎ | 239/289 [05:45<00:50,  1.01s/it]\u001b[A\n",
            " 83%|████████▎ | 240/289 [05:46<00:56,  1.14s/it]\u001b[A\n",
            " 83%|████████▎ | 241/289 [05:48<01:05,  1.36s/it]\u001b[A\n",
            " 84%|████████▎ | 242/289 [05:50<01:11,  1.52s/it]\u001b[A\n",
            " 84%|████████▍ | 243/289 [05:51<01:10,  1.53s/it]\u001b[A\n",
            " 84%|████████▍ | 244/289 [05:52<01:02,  1.39s/it]\u001b[A\n",
            " 85%|████████▍ | 245/289 [05:53<00:50,  1.16s/it]\u001b[A\n",
            " 85%|████████▌ | 246/289 [05:54<00:43,  1.01s/it]\u001b[A\n",
            " 85%|████████▌ | 247/289 [05:54<00:37,  1.11it/s]\u001b[A\n",
            " 86%|████████▌ | 248/289 [05:55<00:33,  1.22it/s]\u001b[A\n",
            " 86%|████████▌ | 249/289 [05:56<00:32,  1.22it/s]\u001b[A\n",
            " 87%|████████▋ | 250/289 [05:57<00:33,  1.17it/s]\u001b[A\n",
            " 87%|████████▋ | 251/289 [05:58<00:42,  1.11s/it]\u001b[A\n",
            " 87%|████████▋ | 252/289 [06:00<00:46,  1.25s/it]\u001b[A\n",
            " 88%|████████▊ | 253/289 [06:02<00:49,  1.37s/it]\u001b[A\n",
            " 88%|████████▊ | 254/289 [06:02<00:41,  1.18s/it]\u001b[A\n",
            " 88%|████████▊ | 255/289 [06:04<00:43,  1.29s/it]\u001b[A\n",
            " 89%|████████▊ | 256/289 [06:06<00:46,  1.40s/it]\u001b[A\n",
            " 89%|████████▉ | 257/289 [06:07<00:47,  1.49s/it]\u001b[A\n",
            " 89%|████████▉ | 258/289 [06:08<00:42,  1.36s/it]\u001b[A\n",
            " 90%|████████▉ | 259/289 [06:09<00:38,  1.27s/it]\u001b[A\n",
            " 90%|████████▉ | 260/289 [06:10<00:34,  1.21s/it]\u001b[A\n",
            " 90%|█████████ | 261/289 [06:11<00:32,  1.14s/it]\u001b[A\n",
            " 91%|█████████ | 262/289 [06:12<00:28,  1.07s/it]\u001b[A\n",
            " 91%|█████████ | 263/289 [06:13<00:24,  1.04it/s]\u001b[A\n",
            " 91%|█████████▏| 264/289 [06:15<00:28,  1.15s/it]\u001b[A\n",
            " 92%|█████████▏| 265/289 [06:17<00:33,  1.40s/it]\u001b[A\n",
            " 92%|█████████▏| 266/289 [06:19<00:36,  1.58s/it]\u001b[A\n",
            " 92%|█████████▏| 267/289 [06:20<00:36,  1.66s/it]\u001b[A\n",
            " 93%|█████████▎| 268/289 [06:22<00:35,  1.70s/it]\u001b[A\n",
            " 93%|█████████▎| 269/289 [06:25<00:39,  1.98s/it]\u001b[A\n",
            " 93%|█████████▎| 270/289 [06:27<00:37,  1.99s/it]\u001b[A\n",
            " 94%|█████████▍| 271/289 [06:29<00:34,  1.92s/it]\u001b[A\n",
            " 94%|█████████▍| 272/289 [06:30<00:31,  1.83s/it]\u001b[A\n",
            " 94%|█████████▍| 273/289 [06:32<00:28,  1.78s/it]\u001b[A\n",
            " 95%|█████████▍| 274/289 [06:33<00:25,  1.73s/it]\u001b[A\n",
            " 95%|█████████▌| 275/289 [06:35<00:22,  1.62s/it]\u001b[A\n",
            " 96%|█████████▌| 276/289 [06:36<00:20,  1.56s/it]\u001b[A\n",
            " 96%|█████████▌| 277/289 [06:38<00:19,  1.65s/it]\u001b[A\n",
            " 96%|█████████▌| 278/289 [06:40<00:18,  1.70s/it]\u001b[A\n",
            " 97%|█████████▋| 279/289 [06:42<00:17,  1.77s/it]\u001b[A\n",
            " 97%|█████████▋| 280/289 [06:45<00:20,  2.26s/it]\u001b[A\n",
            " 97%|█████████▋| 281/289 [06:49<00:21,  2.67s/it]\u001b[A\n",
            " 98%|█████████▊| 282/289 [06:53<00:20,  2.96s/it]\u001b[A\n",
            " 98%|█████████▊| 283/289 [06:54<00:15,  2.52s/it]\u001b[A\n",
            " 98%|█████████▊| 284/289 [06:55<00:10,  2.10s/it]\u001b[A\n",
            " 99%|█████████▊| 285/289 [06:56<00:06,  1.65s/it]\u001b[A\n",
            " 99%|█████████▉| 286/289 [06:57<00:04,  1.42s/it]\u001b[A\n",
            " 99%|█████████▉| 287/289 [06:58<00:02,  1.31s/it]\u001b[A\n",
            "100%|█████████▉| 288/289 [06:59<00:01,  1.38s/it]\u001b[A\n",
            "100%|██████████| 289/289 [07:01<00:00,  1.46s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modern: a box of comfits\n",
            "prizes alice had no idea what to do and in despair she put her hand in her pocket and pulled out a box of comfits luckily the salt water had not got into it and handed them round as prizes \n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    624\u001b[0m         \"\"\"\n\u001b[0;32m--> 625\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-ec27dd495b15>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# print(question)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mans\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0manswerquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-13-ec27dd495b15>\u001b[0m in \u001b[0;36manswerquery\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bert-large-uncased-whole-word-masking-finetuned-squad'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mwhile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mquestion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"enter the question\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_sent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_sent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Modern: \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xW8Q69lzrb8p"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}