{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_Q_A.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/varmach9/QA_NLP/blob/main/NLP_Q_A2222.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TlP2HPbXK3gj",
        "outputId": "045418eb-4315-4295-ed8c-8ec17c124b4a"
      },
      "source": [
        "!git clone https://github.com/susmitmishra125/QA_NLP"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'QA_NLP'...\n",
            "remote: Enumerating objects: 109, done.\u001b[K\n",
            "remote: Counting objects: 100% (109/109), done.\u001b[K\n",
            "remote: Compressing objects: 100% (84/84), done.\u001b[K\n",
            "remote: Total 109 (delta 55), reused 47 (delta 20), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (109/109), 270.17 KiB | 1.04 MiB/s, done.\n",
            "Resolving deltas: 100% (55/55), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2FVm2TnyVV0M",
        "outputId": "cb1aa9ea-ff03-437c-f9cf-01aac4c10ecb"
      },
      "source": [
        "!pip install contractions\n",
        "!pip install transformers"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting contractions\n",
            "  Downloading contractions-0.0.58-py2.py3-none-any.whl (8.0 kB)\n",
            "Collecting textsearch>=0.0.21\n",
            "  Downloading textsearch-0.0.21-py2.py3-none-any.whl (7.5 kB)\n",
            "Collecting pyahocorasick\n",
            "  Downloading pyahocorasick-1.4.2.tar.gz (321 kB)\n",
            "\u001b[K     |████████████████████████████████| 321 kB 4.7 MB/s \n",
            "\u001b[?25hCollecting anyascii\n",
            "  Downloading anyascii-0.3.0-py3-none-any.whl (284 kB)\n",
            "\u001b[K     |████████████████████████████████| 284 kB 41.5 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyahocorasick\n",
            "  Building wheel for pyahocorasick (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyahocorasick: filename=pyahocorasick-1.4.2-cp37-cp37m-linux_x86_64.whl size=85451 sha256=c0b782a33145ffe79e79849cd013392737b15b314151011c4aed7e515dd3da7e\n",
            "  Stored in directory: /root/.cache/pip/wheels/25/19/a6/8f363d9939162782bb8439d886469756271abc01f76fbd790f\n",
            "Successfully built pyahocorasick\n",
            "Installing collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.0 contractions-0.0.58 pyahocorasick-1.4.2 textsearch-0.0.21\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.12.5-py3-none-any.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 4.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 38.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.1.2-py3-none-any.whl (59 kB)\n",
            "\u001b[K     |████████████████████████████████| 59 kB 6.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 35.1 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 46.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.1.2 pyyaml-6.0 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.12.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-UdxUP1rTmP",
        "outputId": "550f3a6d-c95e-48a9-ae09-9f6fc048a75c"
      },
      "source": [
        "%cd QA_NLP"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/QA_NLP\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25hyciYQrcU0",
        "outputId": "bc909d16-e2eb-4347-c814-c63cc920fb1d"
      },
      "source": [
        "# Importing modules\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from transformers import BertTokenizer\n",
        "from transformers import BertForQuestionAnswering\n",
        "import torch\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "import contractions\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "from collections import defaultdict\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "nltk.download('punkt')  # For tokenizers\n",
        "nltk.download('stopwords')\n",
        "\n",
        "\n",
        "# importing dataset\n",
        "data = open('passage.txt', 'r', encoding=\"utf-8\").read()\n",
        "questions = open('question_list.txt', 'r').readlines()\n",
        "\n",
        "# preprocess for modern method\n",
        "\n",
        "\n",
        "def preprocessing(rawReadCorpus, complete_preprocess=False):\n",
        "    pattern = \"^a-zA-Z0-9_\"\n",
        "    rawReadCorpus = contractions.fix(rawReadCorpus)\n",
        "    text_sent = sent_tokenize(rawReadCorpus)  # to split the sentences\n",
        "    text_sent = [sent.lower() for sent in text_sent]  # to convert to lowercase\n",
        "    text_sent = [\"\".join([char for char in text if char not in string.punctuation])\n",
        "                 for text in text_sent]  # removed punctuation\n",
        "    text_sent = [word_tokenize(sent) for sent in text_sent]\n",
        "    # text_sent = [\"\".join([char for char in text if char not in ]) for text in text_sent] # removed punctuation\n",
        "    for i in range(len(text_sent)):\n",
        "        sent = \" \".join(text_sent[i])\n",
        "        sent = re.sub(pattern, ' ', sent)\n",
        "        sent = sent.replace(\"“\", \" \")\n",
        "        sent = sent.replace(\"”\", \" \")\n",
        "        sent = sent.replace(\"—\", \" \")\n",
        "        sent = sent.replace(\"_\", \" \")\n",
        "        text_sent[i] = sent.split(' ')\n",
        "        text_sent[i] = [i for i in text_sent[i] if i != '']\n",
        "        # print(text_sent[i])\n",
        "    if complete_preprocess:\n",
        "        ps = nltk.porter.PorterStemmer()\n",
        "        for i in range(len(text_sent)):\n",
        "            text_sent[i] = [ps.stem(j) for j in text_sent[i]]\n",
        "    i = len(text_sent)-1\n",
        "    while(i >= 0):\n",
        "        if 'chapter' in text_sent[i]:\n",
        "            # print(text_sent[i])\n",
        "            del(text_sent[i])\n",
        "        i -= 1\n",
        "    return text_sent\n",
        "\n",
        "\n",
        "def answer_question(question, answer_text, model, tokenizer):\n",
        "    '''\n",
        "    Takes a question string and an answer_text string (which contains the\n",
        "    answer), and identifies the words within the answer_text that are the\n",
        "    answer. Prints them out.\n",
        "    '''\n",
        "    input_ids = tokenizer.encode(question, answer_text)\n",
        "    sep_index = input_ids.index(tokenizer.sep_token_id)\n",
        "    num_seg_a = sep_index + 1\n",
        "    num_seg_b = len(input_ids) - num_seg_a\n",
        "    segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
        "    assert len(segment_ids) == len(input_ids)\n",
        "    outputs = model(torch.tensor([input_ids]),  # The tokens representing our input text.\n",
        "                    # The segment IDs to differentiate question from answer_text\n",
        "                    token_type_ids=torch.tensor([segment_ids]),\n",
        "                    return_dict=True)\n",
        "    start_scores = outputs.start_logits\n",
        "    end_scores = outputs.end_logits\n",
        "    answer_start = torch.argmax(start_scores)\n",
        "    answer_end = torch.argmax(end_scores)\n",
        "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "    answer = tokens[answer_start]\n",
        "    for i in range(answer_start + 1, answer_end + 1):\n",
        "        if tokens[i][0:2] == '##':\n",
        "            answer += tokens[i][2:]\n",
        "        else:\n",
        "            answer += ' ' + tokens[i]\n",
        "    return (answer, start_scores[0][answer_start], end_scores[0][answer_end])\n",
        "\n",
        "\n",
        "def query(question, text_sent, model, tokenizer, n_sent=3):\n",
        "    outputs = []\n",
        "    sent_count = len(text_sent)\n",
        "    ans = 0\n",
        "    ans_text = 'NA'\n",
        "    for i in tqdm(range(sent_count-n_sent+1)):\n",
        "        segment = text_sent[i:i+n_sent]\n",
        "        passage = ''\n",
        "        for sent in segment:\n",
        "            passage += ' '.join(sent)+'. '\n",
        "        output = answer_question(question, passage, model, tokenizer)\n",
        "\n",
        "        outputs.append((output[0], 2*(output[1]*output[2])/(output[1]+output[2])))\n",
        "        if(len(outputs)>10):\n",
        "            outputs.sort(key=lambda x: x[1], reverse=True)\n",
        "            outputs=outputs[:10]\n",
        "\n",
        "    cnt = Counter()\n",
        "    for output in outputs:\n",
        "        cnt[output[0]] += 1\n",
        "    return cnt.most_common()[0][0]\n",
        "\n",
        "\n",
        "def modern():\n",
        "    text_sent = preprocessing(data)\n",
        "    model = BertForQuestionAnswering.from_pretrained(\n",
        "        'bert-large-uncased-whole-word-masking-finetuned-squad')\n",
        "    tokenizer = BertTokenizer.from_pretrained(\n",
        "        'bert-large-uncased-whole-word-masking-finetuned-squad')\n",
        "    for i in range(len(questions)):\n",
        "        # question in questions:\n",
        "        questions[i] = questions[i].replace('\\n', '')\n",
        "        print(questions[i])\n",
        "        ans = query(questions[i], text_sent=text_sent,model = model,tokenizer = tokenizer)\n",
        "        print(ans)\n",
        "\n",
        "from numpy import dot\n",
        "from numpy.linalg import norm\n",
        "\n",
        "# cos_sim = dot(a, b)/(norm(a)*norm(b))\n",
        "def traditional():\n",
        "    print(\"starting preprocessing\")\n",
        "    text_sent = preprocessing(data,complete_preprocess=False)\n",
        "    text_sent = [' '.join(sent) for sent in text_sent]\n",
        "    print(\"complete preprocessing\")\n",
        "    vectorizer = CountVectorizer(ngram_range=(1, 3))\n",
        "    \n",
        "    sentence_vectors = vectorizer.fit_transform(text_sent).toarray()\n",
        "\n",
        "    for question in questions:\n",
        "        ans = \"NA\"\n",
        "        max_cos = 0\n",
        "        q_vec = vectorizer.transform([question]).toarray()[0]\n",
        "        for i in range(len(text_sent)):\n",
        "            ans_vec = sentence_vectors[i]\n",
        "            cos_sim = dot(ans_vec,q_vec)/(norm(ans_vec)*norm(q_vec)+1e-9)\n",
        "            if cos_sim>max_cos:\n",
        "                # print(sum(q_vec),sum(ans_vec))\n",
        "                ans = text_sent[i]\n",
        "                max_cos = cos_sim\n",
        "        print(question)\n",
        "        print(ans,'\\n')\n",
        "    # print(sentence_vectors.toarr)\n",
        "# traditional()\n",
        "# modern()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vi8-JvFjzuP8"
      },
      "source": [
        "# This function runs the modern method on question present in passage.txt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YDYC-Yyyx7C"
      },
      "source": [
        "modern()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLp3KGYt0APR"
      },
      "source": [
        "# This function runs the traditional method on questions present in passage.txt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LIJf7r3y_Zt"
      },
      "source": [
        "traditional()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yf0W7RG40M3a"
      },
      "source": [
        "# For Testing new questions  (print \"end\" to end testing)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWVeh7PbrcSy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fbadaa0-0624-498d-eec7-15e8e746de7b"
      },
      "source": [
        "def answerquery():\n",
        "    text_sent = preprocessing(data)\n",
        "    model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
        "    while(True):\n",
        "        question = input(\"enter the question\\n\")\n",
        "        if question==\"end\":\n",
        "          break\n",
        "        ans = query(question, text_sent=text_sent,model = model,tokenizer = tokenizer)\n",
        "        print(\"Modern: \"+ans)\n",
        "        vectorizer = CountVectorizer(ngram_range=(1, 3))\n",
        "        text_sent = [' '.join(sent) for sent in text_sent]\n",
        "        sentence_vectors = vectorizer.fit_transform(text_sent).toarray()\n",
        "        ans = \"NA\"\n",
        "        max_cos = 0\n",
        "        q_vec = vectorizer.transform([question]).toarray()[0]\n",
        "        for i in range(len(text_sent)):\n",
        "            ans_vec = sentence_vectors[i]\n",
        "            cos_sim = dot(ans_vec,q_vec)/(norm(ans_vec)*norm(q_vec)+1e-9)\n",
        "            if cos_sim>max_cos:\n",
        "                # print(sum(q_vec),sum(ans_vec))\n",
        "                ans = text_sent[i]\n",
        "                max_cos = cos_sim\n",
        "        # print(question)\n",
        "        print(ans,'\\n')\n",
        "answerquery()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "enter the question\n",
            "end\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xW8Q69lzrb8p"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}